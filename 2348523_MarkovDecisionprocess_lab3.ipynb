{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov decision process  is one of the decision making problem where the outcomes of the state are partially random or partially controllable.\n",
    "\n",
    "we have \n",
    "1.States: A set of possible situations\n",
    "\n",
    "2.Actions: A set of possible actions available to the agent\n",
    "\n",
    "3.Transition model: A function that describes the probability of transitioning from one state to another given an\n",
    "action\n",
    "\n",
    "4.Reward function: A function that describes the reward received by the agent after taking an action in a state\n",
    "\n",
    "\n",
    "5.Policy: A mapping from states to actions\n",
    "6.Value function: A function that describes the value of being in a state, given a policy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation at state: A\n",
      "Time step 1: State = A, Action = right\n",
      "Transition to state: C with reward: 2\n",
      "Time step 2: State = C, Action = left\n",
      "Transition to state: A with reward: 0\n",
      "Time step 3: State = A, Action = right\n",
      "Transition to state: C with reward: 2\n",
      "Time step 4: State = C, Action = left\n",
      "Transition to state: A with reward: 0\n",
      "Time step 5: State = A, Action = right\n",
      "Transition to state: C with reward: 2\n",
      "Time step 6: State = C, Action = left\n",
      "Transition to state: A with reward: 0\n",
      "Time step 7: State = A, Action = right\n",
      "Transition to state: C with reward: 2\n",
      "Time step 8: State = C, Action = left\n",
      "Transition to state: A with reward: 0\n",
      "Time step 9: State = A, Action = right\n",
      "Transition to state: C with reward: 2\n",
      "Time step 10: State = C, Action = left\n",
      "Transition to state: A with reward: 0\n",
      "Total discounted reward: 6.8560164200000004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Mdp:\n",
    "    def __init__(self, states, actions, transition_prob, rewards, policy, gamma=0.9):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_prob = transition_prob\n",
    "        self.rewards = rewards\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def simulate(self, start_state, horizon=10):\n",
    "        state = start_state\n",
    "        total_reward = 0\n",
    "        print(f\"Starting simulation at state: {state}\")\n",
    "\n",
    "        for t in range(horizon):\n",
    "            action = self.policy[state]\n",
    "            print(f\"Time step {t + 1}: State = {state}, Action = {action}\")\n",
    "\n",
    "            next_states = self.states  # Get the list of next states\n",
    "            probabilities = [self.transition_prob.get((state, action, s_prime), 0) for s_prime in next_states]\n",
    "            next_state = np.random.choice(next_states, p=probabilities / np.sum(probabilities))  # Normalize probabilities\n",
    "\n",
    "            # Get the reward for the transition\n",
    "            reward = self.rewards.get((state, action, next_state), 0)  # Default to 0 if not found\n",
    "            print(f\"Transition to state: {next_state} with reward: {reward}\")\n",
    "\n",
    "            # Accumulate discounted reward\n",
    "            total_reward += (self.gamma ** t) * reward\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "        print(f\"Total discounted reward: {total_reward}\")\n",
    "        return total_reward\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "states = ['A', 'B', 'C']\n",
    "actions = ['left', 'right']\n",
    "\n",
    "# Define transition probabilities P(s' | s, a)\n",
    "transition_prob = {\n",
    "    ('A', 'left', 'A'): 0.8, ('A', 'left', 'B'): 0.2, ('A', 'right', 'C'): 1.0,\n",
    "    ('B', 'left', 'A'): 1.0, ('B', 'right', 'C'): 1.0,\n",
    "    ('C', 'left', 'A'): 1.0, ('C', 'right', 'B'): 1.0\n",
    "}\n",
    "\n",
    "# Define rewards R(s, a, s')\n",
    "rewards = {\n",
    "    ('A', 'left', 'A'): 1, ('A', 'left', 'B'): -1, ('A', 'right', 'C'): 2,\n",
    "    ('B', 'left', 'A'): 0, ('B', 'right', 'C'): 1,\n",
    "    ('C', 'left', 'A'): 0, ('C', 'right', 'B'): 1\n",
    "}\n",
    "\n",
    "# Define an arbitrary policy Ï€(s) -> a\n",
    "policy = {\n",
    "    'A': 'right',\n",
    "    'B': 'right',\n",
    "    'C': 'left'\n",
    "}\n",
    "\n",
    "# Initialize MDP with the states, actions, transition probabilities, rewards, and policy\n",
    "mdp = Mdp(states, actions, transition_prob, rewards, policy, gamma=0.9)\n",
    "\n",
    "# Simulate the MDP starting from state 'A' for 10 time steps\n",
    "total_reward = mdp.simulate(start_state='A', horizon=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when it comes to MDP  we need to understand the markov process which is defind by states and transition probability where we have sequence of states s1,s2,....sn and transition probability is defined as the prbability of jumping from a state  from the current state \n",
    " \n",
    " \n",
    "\n",
    "\n",
    "Markov reward process\n",
    "the Markov reward process is defined by the reward ,Transistion probability and gamma being the discount factor\n",
    "reward State R is the expected reward over all the possible states that one can transition to from state s. This reward is received for being at the state S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: [1 0 0]\n",
      "Value Function: [34.8225197  27.58139372  0.        ]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

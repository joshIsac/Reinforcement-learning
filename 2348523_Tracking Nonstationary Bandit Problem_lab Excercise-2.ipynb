{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Tracking non-stationary Bandit problem \n",
    "what do we understand from the term non stationary bandit problem : it is different type of variant of a bandit problem where reward of each distribution changes over time it makes the probability distribution evolve because the strategies taken to achieve the reward wont be suitable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import math \n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we know non stationary bandit problem can be also defined as the reward sequence changing over time \n",
    "when it comes to non stationary bandit problem an agent or an player must balance exploration and exploitation \n",
    "exploration  can be defined as an agent to improve its current knowledge about each action,Improving the accuracy of the estimated action-values, enables an agent to make more informed decisions in the future\n",
    "where as exploitation refers to choosing the best reward out of all the eisting reward that are taken into conwsideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of each arm pulled: [ 27. 383.  36. 554.]\n",
      "total reward for each arm pulled: [  6. 250.   9. 306.]\n",
      "Estimated reward probability for each arm: [0.22222214 0.6527415  0.24999993 0.55234656]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JOSHWIN ISAC\\AppData\\Local\\Temp\\ipykernel_16320\\1422725729.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.argmax(self.rewards/self.counts + 1e-5)\n"
     ]
    }
   ],
   "source": [
    "#non stationary bandit problem using Epsilon Greedy \n",
    "class EpsilonGreedy:\n",
    "    def __init__(self,n_arms,epsilon=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts=np.zeros(n_arms)\n",
    "        self.rewards=np.zeros(n_arms)\n",
    "\n",
    "    def arms(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            #explore\n",
    "            return np.random.randint(0,self.n_arms)\n",
    "        else:\n",
    "            #exploit\n",
    "            return np.argmax(self.rewards/self.counts + 1e-5)\n",
    "        \n",
    "    def update(self,chosen_arm,reward):\n",
    "        self.counts[chosen_arm]+=1\n",
    "        self.rewards[chosen_arm]+=reward\n",
    "\n",
    "n_arms=4\n",
    "epsilon = 0.1\n",
    "bandit=EpsilonGreedy(n_arms,epsilon)\n",
    "\n",
    "probabilities=np.random.uniform(0.3, 0.7, n_arms)\n",
    "for i in range(1000):\n",
    "        arm=bandit.arms()\n",
    "\n",
    "        #introducing non stationarity\n",
    "        if i%100==0:\n",
    "            probabilities+=np.random.uniform(-0.1, 0.1, n_arms)\n",
    "            probabilities=np.clip(probabilities,0,1)#keeping the prababilites within the range of 0,1\n",
    "             \n",
    "        reward=np.random.binomial(1,probabilities[arm])\n",
    "        bandit.update(arm,reward)\n",
    "\n",
    "\n",
    "\n",
    "print(\"counts of each arm pulled:\",bandit.counts)\n",
    "print(\"total reward for each arm pulled:\",bandit.rewards)\n",
    "\n",
    "#calculation of probability \n",
    "prob=bandit.rewards/(bandit.counts+ 1e-5)\n",
    "print(\"Estimated reward probability for each arm:\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thompson Sampling is one of heuristic sampling technique in choosing actions adressing the exploration - exploitation dilemma in MAB here in this algorithm action are taken sequentially in manner in order to balance the exploiting which is basically to mazimize the immediate reward and new information exploration leading to future rewards thompson sampling is also known to be as Online Learning where it uses all of the training information ,evaluates those action based on the training information rather than giving instructs by giving correct action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thompson algorithm\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

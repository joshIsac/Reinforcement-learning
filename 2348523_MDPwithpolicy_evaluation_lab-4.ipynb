{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov descision process with policy evaluation \n",
    "**Markov Decision Process** is defined as a stochastic decision making process which uses a mathematical framework for modelling the decision-making of a dynamic system where the outcomes are either random or under the control of a decision maker (agent). \n",
    "\n",
    "A **Markov Decision Process (MDP)** consists of the following components:\n",
    "\n",
    "- **State Space (S)**: A set of possible states of the system.\n",
    "- **Actions (A)**: A list of actions the agent can perform.\n",
    "- **Transition Probability (T)**: This represents the probability of transitioning from one state to another state when an action is performed. \n",
    "Transition Probability written as $ P(s' | s, a)$, which is the probability of ending in state $ s' $ after taking an action $ a $ in state $ s $.\n",
    "- **Reward Function (R)**: The reward attained after transitioning to a new state.\n",
    "- **Discount Factor ($ \\gamma $)**: A value ranging from 0.0 to 1.0 that determines the future rewards compared to immediate rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'numpy.float64'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Initialize and run policy iteration\u001b[39;00m\n\u001b[0;32m     72\u001b[0m mdp \u001b[38;5;241m=\u001b[39m MDP(states, actions, transition_probs, rewards, discount_factor)\n\u001b[1;32m---> 73\u001b[0m optimal_policy, optimal_values \u001b[38;5;241m=\u001b[39m \u001b[43mmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal Policy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimal_policy)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal Value Function:\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimal_values)\n",
      "Cell \u001b[1;32mIn[4], line 46\u001b[0m, in \u001b[0;36mMDP.policy_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpolicy_iteration\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_improvement():\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mMDP.policy_evaluation\u001b[1;34m(self, threshold)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates)):\n\u001b[0;32m     19\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_table[s]\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_table[s] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[s] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_factor \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_table\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     22\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(delta, \u001b[38;5;28mabs\u001b[39m(v \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_table[s]))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delta \u001b[38;5;241m<\u001b[39m threshold:\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates)):\n\u001b[0;32m     19\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_table[s]\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_table[s] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[s] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_factor \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_table\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a, next_state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_probs[s])])\n\u001b[0;32m     22\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(delta, \u001b[38;5;28mabs\u001b[39m(v \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_table[s]))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delta \u001b[38;5;241m<\u001b[39m threshold:\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'numpy.float64'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "class MDP: \n",
    "    def __init__(self, states, actions, transition_probs, rewards, discount_factor=0.9):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_probs = transition_probs\n",
    "        self.rewards = rewards\n",
    "        self.discount_factor = discount_factor\n",
    "        self.value_table = np.zeros(len(states))  # Initialize value table with zeros\n",
    "        self.policy = [0] * len(states) \n",
    "\n",
    "\n",
    "\n",
    "    def policy_evaluation(self,threshold=1e-6):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(len(self.states)):\n",
    "                v = self.value_table[s]\n",
    "                self.value_table[s] = self.rewards[s] + self.discount_factor * np.sum([self.transition_probs[s][a] * self.value_table[self.states.index(next_state)]\n",
    "                for a, next_state in zip(self.actions, self.transition_probs[s])])\n",
    "                delta = max(delta, abs(v - self.value_table[s]))\n",
    "                if delta < threshold:\n",
    "                    break\n",
    "    \n",
    "\n",
    "    def policy_improvement(self):\n",
    "        policy_stable=True\n",
    "        for s in range(len(self.states)):\n",
    "            oldaction=self.policy[s]\n",
    "            # Compute action-values for all actions in the state\n",
    "            action_values = []\n",
    "            for a in self.actions:\n",
    "                action_value = sum([p * (r + self.discount_factor * self.value_table[s_next])\n",
    "                                    for p, s_next, r in self.transition_probs[s][a]])\n",
    "                action_values.append(action_value)\n",
    "            best_action = np.argmax(action_values)\n",
    "            self.policy[s] = best_action\n",
    "            if oldaction != best_action:\n",
    "                policy_stable = False\n",
    "        return policy_stable\n",
    "    \n",
    "\n",
    "    def policy_iteration(self):\n",
    "        while True:\n",
    "            self.policy_evaluation()\n",
    "            if self.policy_improvement():\n",
    "                break\n",
    "        return self.policy, self.value_table\n",
    "    \n",
    "states = [0, 1, 2]  # Example states\n",
    "actions = [0, 1]    # Example actions\n",
    "# Transition probabilities and rewards in the form {state: {action: [(probability, next_state, reward), ...]}}\n",
    "transition_probs = {\n",
    "    0: {\n",
    "        0: [(1.0, 0, 0)],  # Stay in the same state with reward 0\n",
    "        1: [(1.0, 1, 1)],  # Move to state 1 with reward 1\n",
    "    },\n",
    "    1: {\n",
    "        0: [(1.0, 2, 2)],\n",
    "        1: [(1.0, 0, 0)],\n",
    "    },\n",
    "    2: {\n",
    "        0: [(1.0, 1, 1)],\n",
    "        1: [(1.0, 2, 2)],\n",
    "    }\n",
    "}\n",
    "rewards = [0, 1, 2]  # Example rewards for each state\n",
    "discount_factor = 0.9\n",
    "\n",
    "# Initialize and run policy iteration\n",
    "mdp = MDP(states, actions, transition_probs, rewards, discount_factor)\n",
    "optimal_policy, optimal_values = mdp.policy_iteration()\n",
    "\n",
    "print(\"Optimal Policy:\", optimal_policy)\n",
    "print(\"Optimal Value Function:\", optimal_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration 0\n",
      "Policy Evaluation Iteration 0\n",
      "Value Table: [5.  0.8 0. ]\n",
      "\n",
      "Policy Evaluation Iteration 1\n",
      "Value Table: [8.366   2.01176 0.     ]\n",
      "\n",
      "Policy Evaluation Iteration 2\n",
      "Value Table: [10.8137552   2.89295187  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 3\n",
      "Value Table: [12.59376278  3.5337546   0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 4\n",
      "Value Table: [13.88818429  3.99974635  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 5\n",
      "Value Table: [14.82948762  4.33861554  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 6\n",
      "Value Table: [15.5140034   4.58504122  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 7\n",
      "Value Table: [16.01178327  4.76424198  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 8\n",
      "Value Table: [16.37376879  4.89455677  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 9\n",
      "Value Table: [16.63700467  4.98932168  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 10\n",
      "Value Table: [16.82842979  5.05823473  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 11\n",
      "Value Table: [16.96763415  5.10834829  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 12\n",
      "Value Table: [17.06886355  5.14479088  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 13\n",
      "Value Table: [17.14247757  5.17129193  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 14\n",
      "Value Table: [17.19600969  5.19056349  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 15\n",
      "Value Table: [17.23493825  5.20457777  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 16\n",
      "Value Table: [17.26324709  5.21476895  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 17\n",
      "Value Table: [17.28383329  5.22217998  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 18\n",
      "Value Table: [17.29880357  5.22756928  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 19\n",
      "Value Table: [17.30968995  5.23148838  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 20\n",
      "Value Table: [17.31760653  5.23433835  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 21\n",
      "Value Table: [17.32336347  5.23641085  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 22\n",
      "Value Table: [17.32754992  5.23791797  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 23\n",
      "Value Table: [17.3305943   5.23901395  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 24\n",
      "Value Table: [17.33280817  5.23981094  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 25\n",
      "Value Table: [17.3344181   5.24039052  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 26\n",
      "Value Table: [17.33558885  5.24081198  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 27\n",
      "Value Table: [17.33644021  5.24111848  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 28\n",
      "Value Table: [17.33705932  5.24134136  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 29\n",
      "Value Table: [17.33750954  5.24150343  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 30\n",
      "Value Table: [17.33783694  5.2416213   0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 31\n",
      "Value Table: [17.33807502  5.24170701  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 32\n",
      "Value Table: [17.33824815  5.24176934  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 33\n",
      "Value Table: [17.33837406  5.24181466  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 34\n",
      "Value Table: [17.33846561  5.24184762  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 35\n",
      "Value Table: [17.3385322   5.24187159  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 36\n",
      "Value Table: [17.33858061  5.24188902  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 37\n",
      "Value Table: [17.33861582  5.2419017   0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 38\n",
      "Value Table: [17.33864143  5.24191091  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 39\n",
      "Value Table: [17.33866004  5.24191762  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 40\n",
      "Value Table: [17.33867358  5.24192249  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 41\n",
      "Value Table: [17.33868343  5.24192603  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 42\n",
      "Value Table: [17.33869059  5.24192861  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 43\n",
      "Value Table: [17.3386958   5.24193049  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 44\n",
      "Value Table: [17.33869958  5.24193185  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 45\n",
      "Value Table: [17.33870234  5.24193284  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 46\n",
      "Value Table: [17.33870434  5.24193356  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 47\n",
      "Value Table: [17.3387058   5.24193409  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 48\n",
      "Value Table: [17.33870685  5.24193447  0.        ]\n",
      "\n",
      "Policy Evaluation Iteration 49\n",
      "Value Table: [17.33870762  5.24193474  0.        ]\n",
      "\n",
      "Optimal Policy: [0, 0, 0]\n",
      "Value Table: [17.33870762  5.24193474  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, states, actions, transition_prob, rewards, discount_factor=0.9):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_probs = transition_prob\n",
    "        self.rewards = rewards\n",
    "        self.discount_factor = discount_factor\n",
    "        self.value_table = np.zeros(len(states))  # Initialize value table with zeros\n",
    "        self.policy = [0] * len(states)           # Initialize policy with zeros\n",
    "\n",
    "    def policy_evaluation(self, threshold=1e-6):\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            delta = 0\n",
    "            print(f\"Policy Evaluation Iteration {iteration}\")\n",
    "            for s in self.states:\n",
    "                v = self.value_table[s]\n",
    "                a = self.policy[s]\n",
    "                # Compute the value for the chosen action under the current policy\n",
    "                self.value_table[s] = sum([p * (r + self.discount_factor * self.value_table[s_next])\n",
    "                                           for p, s_next, r in self.transition_probs[s][a]])\n",
    "                delta = max(delta, abs(v - self.value_table[s]))\n",
    "            print(f\"Value Table: {self.value_table}\\n\")\n",
    "            if delta < threshold:\n",
    "                break\n",
    "            iteration += 1\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        iteration = 0\n",
    "        while not policy_stable:\n",
    "            policy_stable = True\n",
    "            print(f\"Policy Improvement Iteration {iteration}\")\n",
    "            for s in self.states:\n",
    "                old_action = self.policy[s]\n",
    "                # Compute action-values for all actions in the state\n",
    "                action_values = []\n",
    "                for a in self.actions:\n",
    "                    action_value = sum([p * (r + self.discount_factor * self.value_table[s_next])\n",
    "                                        for p, s_next, r in self.transition_probs[s][a]])\n",
    "                    action_values.append(action_value)\n",
    "                best_action = np.argmax(action_values)  # Choose the best action based on the action-values\n",
    "                self.policy[s] = best_action\n",
    "                if old_action != best_action:\n",
    "                    policy_stable = False\n",
    "            print(f\"Policy: {self.policy}\")\n",
    "            print(f\"Value Table: {self.value_table}\\n\")\n",
    "            iteration += 1\n",
    "        return policy_stable\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            print(f\"Policy Iteration {iteration}\")\n",
    "            self.policy_evaluation()\n",
    "            if self.policy_improvement():\n",
    "                break\n",
    "            iteration += 1\n",
    "        return self.policy, self.value_table\n",
    "\n",
    "# Define states, actions, transition probabilities, and rewards\n",
    "states = [0, 1, 2]  # State indices\n",
    "actions = [0, 1]    # Action indices, e.g., 0 and 1 could represent 'a1' and 'a2'\n",
    "\n",
    "# Define transition probabilities and rewards\n",
    "# transition_prob[state][action] = [(probability, next_state, reward), ...]\n",
    "transition_prob = {\n",
    "    0: {0: [(0.7, 0, 5), (0.3, 1, 5)], 1: [(1.0, 2, 10)]},\n",
    "    1: {0: [(0.4, 0, -1), (0.6, 2, -1)], 1: [(1.0, 1, 2)]},\n",
    "    2: {0: [(1.0, 2, 0)], 1: [(0.5, 0, 3), (0.5, 1, 3)]}\n",
    "}\n",
    "\n",
    "# Rewards can be accessed from the transition_probs directly for simplicity.\n",
    "\n",
    "# Initialize and solve the MDP\n",
    "mdp = MDP(states, actions, transition_prob, rewards=None)\n",
    "optimal_policy, value_table = mdp.policy_iteration()\n",
    "\n",
    "print(\"Optimal Policy:\", optimal_policy)\n",
    "print(\"Value Table:\", value_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we see that that the policy iteration remains unchanged i.e converging towards stable states  and the policy does not change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation of MDP using Grid Based \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self,grid,transition_states,rewards,discount=0.9):\n",
    "        self.grid = grid\n",
    "        self.transition_states = transition_states\n",
    "        self.rewards = rewards\n",
    "        self.discount = discount\n",
    "        self.action =[\"up\", \"down\", \"left\", \"right\"]\n",
    "    \n",
    "\n",
    "    def transition_probabilities(self):\n",
    "        # Transition probabilities for each state-action pair\n",
    "        transition_probabilities = {}\n",
    "        for row in range(self.grid_size):\n",
    "            for col in range(self.grid_size):\n",
    "                state = (row, col)\n",
    "                transition_probabilities[state] = {}\n",
    "                for action in self.actions:\n",
    "                    transition_probabilities[state][action] = self.get_transitioning_state(state, action)\n",
    "        return transition_probabilities\n",
    "    \n",
    "\n",
    "    def transition_states(self, state, action):\n",
    "    # Transitioning state for a given state-action pair\n",
    "        if action == \"up\" and state[0] > 0:\n",
    "            return (state[0] - 1, state[1])\n",
    "        elif action == \"down\" and state[0] < self.grid_size - 1:\n",
    "            return (state[0] + 1, state[1])\n",
    "        elif action == \"left\" and state[1] > 0:\n",
    "            return (state[0], state[1] - 1)\n",
    "        elif action == \"right\" and state[1] < self.grid_size - 1:\n",
    "            return (state[0], state[1] + 1)\n",
    "        else:\n",
    "            return state \n",
    "        \n",
    "\n",
    "\n",
    "    def exp_reward(self,state,action):\n",
    "        # Expected reward for a given state-action pair\n",
    "        return self.rewards.get((state, action), 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate action value\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

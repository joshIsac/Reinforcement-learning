{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo \n",
    "\n",
    "Monte Carlo Method is one of the computational technique mainly relies on random sampling to estimate the numerical method or probabilities widely used in problems that have uncertainity ,optimization and approximation \n",
    "\n",
    "\n",
    "\n",
    "in Reinforcement Learning it is a model free algorithm that mainly rely on sampled experience to estimate the value functions and optimize the policies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Idea \n",
    "*It uses randomness which is deterministic in nature.\n",
    "*it generates large numbers of random sample using average to approx the quantities or say estimate the value of the value function and update the policies \n",
    "\n",
    "\n",
    "\n",
    "Mathematical Intuition\n",
    "*The law of large numbers is a mathematical concept that states that the average of a large number of independent random variables converging to expected value\n",
    "\n",
    "\n",
    "State value function VœÄ(s)=EœÄ[Gt‚à£St=s]\n",
    "which is expected return starting from state s under policy ùúã.\n",
    "\n",
    "\n",
    "Action-value function:\n",
    "QœÄ(s,a)=EœÄ[Gt‚à£St=s,A t=a]\n",
    "the expected return starting from state s, taking action a,and following œÄ.\n",
    "\n",
    "and the total return is defined by \n",
    "Gt=Rt+1+Œ≥(Rt+2)+Œ≥^2(Rt+1)+....Œ≥^n(Rt+n+1)\n",
    "where Œ≥ is the discounted factor ranging from [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm for MC Control\n",
    "Initialize \n",
    "*Q(s,a) arbitrarily and a policy œÄ.\n",
    "*Repeat for many episodes:\n",
    "\n",
    "*Generate an episode by following œÄ.\n",
    "\n",
    "*For each state-action pair (s,a) in the episode:Compute the return ùê∫ùë°.\n",
    "\n",
    "*Update Q(s,a) as the average of all returns observed after\n",
    "\n",
    "\n",
    "Q(s,a)‚ÜêQ(s,a)+Œ±(Gt‚àíQ(s,a)).\n",
    "*Update œÄ(s) as the greedy policy with respect to Q(s,a).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import seaborn as sn \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP_Grid:\n",
    "    def __init__(self,grid_size,start_state,end_state,rewards,discount_factor=0.9):\n",
    "        self.grid_size = grid_size\n",
    "        self.start_state = start_state\n",
    "        self.end_state = end_state\n",
    "        self.rewards=rewards\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "\n",
    "    def transition_states(self,state,action):\n",
    "        row,col=state\n",
    "        # Transition to next state based on the action taken\n",
    "        if action == 'up':\n",
    "              next_state = (max(0, row - 1), col)\n",
    "        elif action == 'down':\n",
    "            next_state = (min(self.grid_size - 1, row + 1), col)\n",
    "        elif action == 'left':\n",
    "            next_state = (row, max(0, col - 1))\n",
    "        elif action == 'right':\n",
    "            next_state = (row, min(self.grid_size - 1, col + 1))\n",
    "        \n",
    "        # Reward checks from the Grid World\n",
    "        reward = self.rewards.get((state, action), 0)\n",
    "        return next_state, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monte carlo Implementation\n",
    "class Monte_carlo:\n",
    "    def __init__(self,grid_size,start_state,end_state,discount_factor=0.9,epsilon=0.1):\n",
    "        self.grid_size = grid_size\n",
    "        self.start_state = start_state\n",
    "        self.end_state = end_state\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        #initialize the policy with Random experience\n",
    "        self.policy = {(row,col):random.choice(['up','down','left','right'])\n",
    "                       for row in range(grid_size) for col in range(grid_size) \n",
    "                       if(row,col)!=self.start_state and (row,col)!=self.end_state}\n",
    "        \n",
    "\n",
    "        #state action value\n",
    "        self.q_val={}\n",
    "        for row in range(grid_size):\n",
    "            for col in range(grid_size):\n",
    "                state=(row,col)\n",
    "                if state!=self.start_state and state!=self.end_state:\n",
    "                    self.q_val[state]={action:0 for action in ['up','down','left','right']}\n",
    "\n",
    "        \n",
    "        #Counting state-action pair occurrences for returning the average returns\n",
    "        self.counts={}\n",
    "        for row in range(grid_size):\n",
    "            for col in range(grid_size):\n",
    "                state=(row,col)\n",
    "                if state!=self.start_state and state!=self.end_state:\n",
    "                    self.counts[state]={action:0 for action in ['up','down','left','right']}\n",
    "\n",
    "            \n",
    "        def generate_episode(self,decision_model):\n",
    "            episode=[]\n",
    "            state=self.start_state\n",
    "            while state!=self.end_state:\n",
    "                if state not in self.policy:\n",
    "                    break\n",
    "                action=self.policy[state]\n",
    "                next_state,reward =self.transition(state,action,decision_model)\n",
    "                episode.append((state,action,next_state,reward))\n",
    "                state=next_state\n",
    "            return episode\n",
    "        \n",
    "        def update_policy(self):\n",
    "            for state in self.q_val:\n",
    "                for action in self.q_val[state]:\n",
    "                    if self.counts[state][action]==0:\n",
    "                        self.policy[state]=random.choice(['up','down','left','right'])\n",
    "                    else:\n",
    "                        self.policy[state]=max(self.q_val[state],key=self.q_val[state].get)\n",
    "\n",
    "        def mc_update\n",
    "                        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
